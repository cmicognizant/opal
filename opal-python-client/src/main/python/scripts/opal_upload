#!/usr/bin/python

from __future__ import print_function, division

import sys, os, errno
import re
import json
import time
import csv
import shutil
from os import path, listdir
from opal_tools_lib import *

verbose = '-v' in sys.argv
quiet = '-q' in sys.argv


## Parse config file ##

CONFIGFILE = '~/.opal/fileupload.conf'

def setup():
    global file_type, done, src_folder, done_folder, opal_folder

    with setup_loader(CONFIGFILE) as config:

        file_type = config.get('main', 'file_type')
        done = config.get('main', 'done')
        src_folder = expanduser(config.get('main', 'src_folder'))
        done_folder = expanduser(config.get('main', 'done_folder')) if done == 'move' else None
        opal_folder = config.get('main', 'opal_folder')

        if file_type != 'csv':
            raise KnownError(
                'Configuration file error: file_type = {0}. File types other than "csv" are not currently '
                'supported'.format(file_type))
        if done not in ('move', 'delete', 'none'):
            raise KnownError(
                'Configureation file error: done = {0}, should be one of "move", "delete" or "none"'.format(done))

        if not done_folder.endswith('/'):
            done_folder += '/'


class FileNamePatternException (KnownError):
    pass

class OpalFile:

    reindex_tables = []
    operations = ['update', 'delete', 'replace']

    # Retain the order in which the files were processed, but only reindex each project once.
    @classmethod
    def reindex_all(cls):
        logging.info('starting reindex for tables {0}'.format(', '.join(cls.reindex_tables)))
        run_rest_command_params('/indexes', ('table='+u(t) for t in cls.reindex_tables), 'PUT')
        del cls.reindex_tables[:]


    def __init__(self, filename):
        self.name = filename
        self._entity_type = None
        # filename should be in the format <timestamp>.<project_name>.<table_name>.<operation>.csv
        # example: 20140503140523.mdsantwerp.lifelines.update.csv
        match = re.match(r'^([^\.]+)\.([^\.]+)\.([^\.]+)\.([^\.]+)(.*)\.csv$', filename)
        if match:
            self.timestamp, self.project, self.table, self.operation = match.groups()[0:4]
        if not match or self.operation not in self.operations:
            raise FileNamePatternException(
                'File name {0} does not match expected format: '
                '<timestamp>.<project_name>.<table_name>.<operation>.csv\n'
                'Valid operations: update, delete, replace'.format(filename))
        self.path = path.join(src_folder, filename)

    def table_reference(self):
        return self.project + '.' + self.table

    # processes this file
    def process(self):
        logging.info('processing '+self.name)
        start_time = time.time()

        if not self.operation in self.operations:
            raise Exception("Operation '{0}' not implemented".format(self.operation))

        getattr(self, self.operation)()

        getattr(self, 'done_'+done)()

        logging.info('finished processing {0} in {1:.3f} seconds'.format(self.name, time.time() - start_time))

    def update(self):
        self.do_upload()
        try:
            self.do_import()
        finally:
            self.schedule_reindex()
            self.do_remove_uploaded()

    def delete(self):
        # Delete ids are sent as url parameters. Sending a too large url results in an error 413 FULL HEAD,
        # so we need to split large deletions up over several requests.
        with open(self.path) as idfile:
            size = filesize(idfile)
            url = '/datasource/{0}/table/{1}/valueSets'.format(u(self.project), u(self.table))

            if quiet:
                callback = self.schedule_reindex
            elif size:
                def callback():
                    sys.stderr.write('\rDeleting, {0:.0%} completed...'.format(idfile.tell() / size))
                    self.schedule_reindex()
            else:
                def callback():
                    sys.stderr.write('.')
                    self.schedule_reindex()

            if not quiet and not size: sys.stderr.write('Deleting.')

            params = ('id='+u(id) for id in self.delete_ids(idfile))
            run_rest_command_params(url, params, method='DELETE', progresscallback=callback)

            if not quiet: sys.stderr.write('done\n')

    def delete_ids(self, idfile):
        reader = csv.reader(idfile)
        # skip header
        reader.next()
        for l in reader:
            yield l[0]

    def replace(self):
        self.do_upload()
        self.do_truncate()
        self.schedule_reindex()
        self.do_import()
        self.do_remove_uploaded()

    def do_truncate(self):
        logging.info('Truncating table {0}.{1}'.format(self.project, self.table))
        run_rest_command(
            '/datasource/{0}/table/{1}/valueSets'.format(u(self.project), u(self.table)),
            method='DELETE')

    @property
    def entity_type(self):
        if not self._entity_type:
            logging.log(LOGINFO, 'loading entity type for table {0}'.format(self.table))
            res = json.loads(run_rest_command(
                '/datasource/{0}/table/{1}'.format(u(self.project), u(self.table))))
            self._entity_type = res['entityType']
        return self._entity_type

    def do_upload(self):
        return upload_file(self.path, opal_folder)

    def do_import(self):
        # opal import-csv -o <opal_base_url> -u <username> -p <password> -d <project> -i
        #      -pa <opalfs_self_path> -ty <entity_type> -t <table>
        import_cmd = ['opal', 'import-csv'] + auth_params + ['-d', self.project, '-i', '-ncv',
                      '-pa', opal_folder+'/'+self.name, '-ty', self.entity_type, '-t', self.table]
        result = json.loads(run_command(import_cmd))
        job_id = result['id']
        status = result['status']
        percent = result.get('progress', {}).get('percent', 0)
        logging.log(LOGINFO, "Import job ID: "+str(job_id))
        if not quiet: sys.stderr.write('Importing data (job id {0})\n'.format(job_id))
        while status == 'IN_PROGRESS':
            if not quiet: sys.stderr.write('\rprogress {0}%'.format(percent))
            logging.log(LOGINFO, "Progress on job {0}: {1}%".format(job_id, percent))
            time.sleep(1)
            result = json.loads(run_rest_command(
                '/project/{0}/command/{1}'.format(u(self.project), u(job_id))))
            status = result['status']
            percent = result.get('progress', {}).get('percent', 0)
        if not quiet: sys.stderr.write('...done\n')
        if status == 'FAILED':
            err = []
            for msg in result['messages']:
                match = re.match(r'Variables do not exist in (.*) and creating new variables is disabled: (.*)', msg['msg'])
                if match:
                    err.append("variable(s) {0} do not exist in table {1}".format(match.groups()[1], match.groups()[0]))
            if err:
                err.append("make sure the first row of the .csv file is a header row.")
                raise KnownError(', '.join(err))

            raise Exception('Import job failed: {0}\nJob messages:\n{1}'.format(
                self.name, '\n'.join('* '+m['msg'].strip() for m in result['messages'])))

    def do_remove_uploaded(self):
        # opal file -o <url> -u administrator -p password --delete /data_import/sample.csv
        remove_cmd = ['opal', 'file'] + auth_params + ['--delete', opal_folder + '/' + self.name, '-f']
        run_command(remove_cmd)

    def schedule_reindex(self):
        if self.project not in self.reindex_tables:
            self.reindex_tables.append(self.table_reference())

    def done_delete(self):
        logging.info('deleting {0}'.format(self.path))
        os.remove(self.path)

    def done_move(self):
        if not path.isdir(done_folder):
            raise KnownError("done_folder '{0}' does not exist".format(done_folder))
        try:
            destpath = path.join(done_folder, self.name)
            if path.exists(destpath):
                logging.warn('Destination {0} exists, attempting to overwrite'.format(destpath))
                os.remove(destpath)
        except OSError as e:
            logging.error(e)
            logging.log(LOGINFO, e, exc_info=True)
        logging.info('moving {0} to {1}'.format(self.path, done_folder))
        try:
            shutil.move(self.path, done_folder)
        except OSError as e:
            raise KnownError("Cannot move file to done_folder: "+str(e))

    def done_none(self):
        pass


def filesize(file):
    "Get the file size of an open file. The file must be seekable, otherwise returns None."
    try:
        curpos = file.tell()
        file.seek(0, os.SEEK_END)
        size = file.tell()
        file.seek(curpos)
    except IOError as e:
        if e.errno != errno.ESPIPE:
            raise
        size = None
    return size


def upload_file(src, opal_dest):
    "Upload a file specified by src to opal in the opal_dest folder"
    #opal file -o <url> -u administrator -p password -up sample.csv /data_import
    #opal file -o http://localhost:8080 -sc server.crt -sk server.key -up sample.csv /data_import
    logging.info('Uploading '+src)
    upload_cmd = ['opal', 'file'] + auth_params + ['-up', src, opal_dest]
    try:
        run_command(upload_cmd)
    except subprocess.CalledProcessError as e:
        if '404 Not Found' in e.output:
            raise KnownError("Folder '{0}' does not exist on the server".format(opal_dest))
        raise


# returns a list of ImportFile, ordered by date (oldest first)
def get_files():
    list = []
    try:
        files = get_all_files()
    except OSError as e:
        raise KnownError("Can not access src_folder: " + str(e))
    for f in files:
        fullname = path.join(src_folder, f)
        if isfile(fullname):
            try:
                list.append(OpalFile(f))
            except FileNamePatternException:
                logging.log(logging.DEBUG, "File {0} is not a table operation file, skipping".format(f))

    list.sort(key=lambda f: f.name.split('_', 1)[0])
    return list

all_files = None
def get_all_files():
    global all_files
    if all_files == None:
        all_files = set(listdir(src_folder))
    return all_files

def main():
    retcode = 0
    try:
        files = get_files()
        if not files:
            logging.warn('Source directory is empty, nothing to do')
            sys.exit(0)
        for file in files:
            file.process()
    except Exception as e:
        handle_exception(e)
        retcode |= 1
        
    # We try to reindex even if there was an error in one of the uploads, to make sure other changed tables are up to date.
    try:
        OpalFile.reindex_all()
        logging.info('done')
    except Exception as e:
        handle_exception(e)
        retcode |= 2

    sys.exit(retcode)
    

if __name__ == '__main__':
    setup()
    main()
