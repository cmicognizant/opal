#!/usr/bin/python

from __future__ import print_function, division

import sys, os
import re
import logging
import subprocess
import json
import time
from ConfigParser import ConfigParser
import csv
from urllib import quote_plus
import shutil
from StringIO import StringIO
from os import path, listdir
from os.path import expanduser, isfile

verbose = '-v' in sys.argv
quiet = '-q' in sys.argv


# This exception indicates that this is a known error condition handled by the code, so no need to print stacktraces
# and such.
class OpalUploadError (Exception):
    pass

## Parse config file ##

def parse_config():
    global url, file_type, done, src_folder, done_folder, opal_folder, logfile, auth_params

    config = ConfigParser()
    config.read(expanduser('~/.opal/fileupload.conf'))

    url = config.get('main', 'url')
    file_type = config.get('main', 'file_type')
    done = config.get('main', 'done')
    src_folder = expanduser(config.get('main', 'src_folder'))
    done_folder = expanduser(config.get('main', 'done_folder')) if done == 'move' else None
    opal_folder = config.get('main', 'opal_folder')
    logfile = expanduser(config.get('main', 'logfile'))

    if config.has_option('main', 'use_certificate') and \
       config.getboolean('main', 'use_certificate'):
        auth_params = ['-o', url, '-sc', config.get('main', 'ssl_cert'), '-sk', config.get('main', 'ssl_key')]
    else:
        auth_params = ['-o', url, '-u', config.get('main', 'user'), '-p', config.get('main', 'password')]

    if file_type != 'csv':
        logging.error('file_type = {0}. File types other than "csv" are not currently supported'.format(file_type))
        sys.exit(1)
    if done not in ('move', 'delete', 'none'):
        logging.error('done = {0}, should be one of "move", "delete" or "none"'.format(done))
        sys.exit(1)

    if not done_folder.endswith('/'):
        done_folder += '/'

parse_config()

## Configure logging ##

# define an intermediate level for info that should by default be logged to the logfile but not to the command line
LOGFILEINFO = logging.INFO - 5
logging.addLevelName(LOGFILEINFO, 'LOGFILEINFO')
def configure_logging():
    logging.getLogger().setLevel(logging.DEBUG)

    logfilehandler = logging.FileHandler(logfile)
    logfilehandler.setLevel(LOGFILEINFO)
    logfilehandler.setFormatter(logging.Formatter('%(levelname)-7s %(asctime)s: %(message)s'))
    logging.getLogger().addHandler(logfilehandler)

    stderrhandler = logging.StreamHandler()
    stderrhandler.setLevel(logging.INFO)
    if verbose:
        stderrhandler.setLevel(logging.DEBUG)
        logfilehandler.setLevel(logging.DEBUG)
    if quiet:
        stderrhandler.setLevel(logging.WARN)
    logging.getLogger().addHandler(stderrhandler)

configure_logging()


MAX_URL_SIZE = 2000


def u(it):
    return quote_plus(str(it))


class OpalFile:

    reindex_projects = []
    operations = ['update', 'delete', 'replace']

    @staticmethod
    def do_reindex(project):
        logging.info('starting reindex for project {0}'.format(project))
        run_rest_command('/datasource/{0}/indexes'.format(u(project)), method='PUT')

    # Retain the order in which the files were processed, but only reindex each project once.
    @classmethod
    def reindex_all(cls):
        for p in cls.reindex_projects:
            try:
                cls.do_reindex(p)
            except Exception as e:
                logging.error('Error while reindexing project {0}: {1}'.format(p, e))
                logging.debug(e, exc_info=True)
        del cls.reindex_projects[:]


    def __init__(self, filename):
        self.name = filename
        self._entity_type = None
        # filename should be in the format <timestamp>_<project_name>_<table_name>_<operation>.csv
        # example: 20140503140523_mdsantwerp_lifelines_update.csv
        match = re.match(r'^([^_]+)_([^_]+)_([^_]+)_([^_]+)(.*)\.csv$', filename)
        if match:
            self.timestamp, self.project, self.table, self.operation = match.groups()[0:4]
        if not match or self.operation not in self.operations:
            raise Exception('File name {0} does not match expected format: '
                            '<timestamp>_<project_name>_<table_name>_<operation>.csv\n'
                            'Valid operations: update, delete, replace'.format(filename))
        self.path = path.join(src_folder, filename)

    # processes this file
    def process(self):
        logging.info('processing '+self.name)
        start_time = time.time()

        if not self.operation in self.operations:
            raise Exception("Operation '{0}' not implemented".format(self.operation))

        getattr(self, self.operation)()

        getattr(self, 'done_'+done)()

        logging.info('finished processing {0} in {1:.3f} seconds'.format(self.name, time.time() - start_time))

    def update(self):
        self.do_upload()
        try:
            self.do_import()
        finally:
            self.schedule_reindex()
            self.do_remove_uploaded()

    def delete(self):
        # Delete ids are sent as url parameters. Sending a too large url results in an error 413 FULL HEAD,
        # so we need to split large deletions up over several requests.
        base_url = '/datasource/{0}/table/{1}/valueSets?'.format(u(self.project), u(self.table))
        url = StringIO()
        url.write(base_url)

        def send():
            # if no ids return
            if url.len == len(base_url): return
            # strip off last '&'
            run_rest_command(url.getvalue()[:-1], method='DELETE')
            self.schedule_reindex()
            url.seek(0)
            url.truncate()
            url.write(base_url)
            if not quiet: sys.stderr.write('.')

        if not quiet: sys.stderr.write('Deleting')
        for id in self.delete_ids():
            idstring = 'id='+u(id)
            if url.len + len(idstring) > MAX_URL_SIZE:
                send()
            url.write(idstring+'&')
        send()
        if not quiet: sys.stderr.write('\n')

    def delete_ids(self):
        with open(self.path) as f:
            reader = csv.DictReader(f)
            if not 'id' in reader.fieldnames:
                raise OpalUploadError("No column named 'id' found in csv file '{1}'".format(self.path))
            for l in reader:
                yield l['id']

    def replace(self):
        self.do_upload()
        self.do_truncate()
        self.schedule_reindex()
        self.do_import()
        self.do_remove_uploaded()

    def do_truncate(self):
        logging.info('Truncating table {0}:{1}'.format(self.project, self.table))
        run_rest_command(
            '/datasource/{0}/table/{1}/valueSets'.format(u(self.project), u(self.table)),
            method='DELETE')

    @property
    def entity_type(self):
        if not self._entity_type:
            logging.log(LOGFILEINFO, 'loading entity type for table {0}'.format(self.table))
            res = json.loads(run_rest_command(
                '/datasource/{0}/table/{1}'.format(u(self.project), u(self.table))))
            self._entity_type = res['entityType']
        return self._entity_type

    def do_upload(self):
        #opal file -o <url> -u administrator -p password -up sample.csv /data_import
        #opal file -o http://localhost:8080 -sc server.crt -sk server.key -up sample.csv /data_import
        logging.info('Uploading '+self.path)
        upload_cmd = ['opal', 'file'] + auth_params + ['-up', self.path, opal_folder]
        try:
            run_command(upload_cmd)
        except subprocess.CalledProcessError as e:
            if '404 Not Found' in e.output:
                raise OpalUploadError("Folder '{0}' does not exist on the server".format(opal_folder))
            raise
        
    def do_import(self):
        # opal import-csv -o <opal_base_url> -u <username> -p <password> -d <project> -i
        #      -pa <opalfs_self_path> -ty <entity_type> -t <table>
        import_cmd = ['opal', 'import-csv'] + auth_params + ['-d', self.project, '-i', 
                      '-pa', opal_folder+'/'+self.name, '-ty', self.entity_type, '-t', self.table]
        result = json.loads(run_command(import_cmd))
        job_id = result['id']
        status = result['status']
        logging.log(LOGFILEINFO, "Import job ID: "+str(job_id))
        if status == 'IN_PROGRESS':
            if not quiet: sys.stderr.write('Importing data (job id {0})'.format(job_id))
            while status == 'IN_PROGRESS':
                if not quiet: sys.stderr.write('.')
                time.sleep(1)
                result = json.loads(run_rest_command(
                    '/project/{0}/command/{1}'.format(u(self.project), u(job_id))))
                status = result['status']
                if status == 'FAILED':
                    raise Exception('Import job failed: {0}'.format(self.name))
            if not quiet: sys.stderr.write('\n')

    def do_remove_uploaded(self):
        # opal file -o <url> -u administrator -p password --delete /data_import/sample.csv
        remove_cmd = ['opal', 'file'] + auth_params + ['--delete', opal_folder + '/' + self.name, '-f']
        run_command(remove_cmd)

    def schedule_reindex(self):
        if self.project not in self.reindex_projects:
            self.reindex_projects.append(self.project)

    def done_delete(self):
        logging.info('deleting {0}'.format(self.path))
        os.remove(self.path)

    def done_move(self):
        if not path.isdir(done_folder):
            raise OpalUploadError("done_folder '{0}' does not exist".format(done_folder))
        try:
            destpath = path.join(done_folder, self.name)
            if path.exists(destpath):
                logging.warn('Destination {0} exists, attempting to overwrite'.format(destpath))
                os.remove(destpath)
        except OSError as e:
            logging.error(e)
            logging.log(LOGFILEINFO, e, exc_info=True)
        logging.info('moving {0} to {1}'.format(self.path, done_folder))
        try:
            shutil.move(self.path, done_folder)
        except OSError as e:
            raise OpalUploadError("Cannot move file to done_folder: "+str(e))

    def done_none(self):
        pass


def run_rest_command(url, method=None):
    m = []
    if method != None:
        m = ['-m', method]
    return run_command(['opal', 'rest'] + auth_params + ['-v', url] + m)

def run_command(cmd):
    logging.log(LOGFILEINFO, 'executing ' + ' '.join(hide_password(cmd)))
    process = None
    try:
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = process.communicate()
        retcode = process.wait()
        output = err+'\n'+out
        logging.debug(output)
        if retcode != 0:
            logging.log(LOGFILEINFO, 'subcommand exited with return code {0}: {1}'.format(
                retcode, limit_length(' '.join(hide_password(cmd)))))
            err = subprocess.CalledProcessError(retcode, hide_password(cmd))
            err.output = output
            raise err
        return out
    except:
        if process:
            if process.poll() is None:
                process.kill()
            process.wait()
        raise

def hide_password(cmd):
    try:
        password_idx = cmd.index('-p')
        return cmd[:password_idx+1] + ['{password_not_logged}'] + cmd[password_idx+2:]
    except ValueError:
        return cmd

def limit_length(msg, maxlen=1000, end=".....rest of message not shown...."):
    if len(msg) > maxlen:
        msg = msg[:1000-len(end)] + end
    return msg


# returns a list of ImportFile, ordered by date (oldest first)
def get_files():
    list = []
    try:
        files = listdir(src_folder)
    except OSError as e:
        raise OpalUploadError("Can not access src_folder: " + str(e))
    for f in files:
        fullname = path.join(src_folder, f)
        if isfile(fullname):
            if f.endswith('.'+file_type):
                list.append(OpalFile(f))
            else:
                logging.log(LOGFILEINFO, "Skipping file with wrong format {0}".format(f))

    list.sort(key=lambda f: f.name)
    return list


def handle_exception(e):
    if isinstance(e, OpalUploadError):
        logging.error('Error: ' + str(e))
        return

    msg = "Error: {0}: {1}".format(type(e).__name__, e)
    logging.error(msg)
    if not verbose:
        print("Run '{0} -v' for more information".format(sys.argv[0]), file=sys.stderr)
    logging.debug(e, exc_info=True)

def main():
    retcode = 0
    try:
        files = get_files()
        if not files:
            logging.warn('Source directory is empty, nothing to do')
            sys.exit(0)
        for file in files:
            file.process()
    except Exception as e:
        handle_exception(e)
        retcode |= 1
        
    # We try to reindex even if there was an error in one of the uploads, to make sure other changed tables are up to date.
    try:
        OpalFile.reindex_all()
        logging.info('done')
    except Exception as e:
        handle_exception(e)
        retcode |= 2

    sys.exit(retcode)
    

if __name__ == '__main__':
    main()
